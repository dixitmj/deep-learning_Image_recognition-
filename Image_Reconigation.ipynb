{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjEPPOMdA5t1bDizt7CQ7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dixitmj/deep-learning_Image_recognition-/blob/main/Image_Reconigation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe0kFoqXITDw",
        "outputId": "a1af9540-8244-4cc7-99f1-67b2deccdd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "import pathlib\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import layers,models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the path to the \"vehicle_photos\" folder in your Google Drive\n",
        "loaded_data = Path(\"/content/drive/My Drive/DL\")\n",
        "\n",
        "# Initialize a counter to keep track of the total number of images\n",
        "total_image_count = 0\n",
        "\n",
        "# Check if there are any subfolders (flower categories)\n",
        "vehicle_categories = list(loaded_data.glob('*'))\n",
        "if len(vehicle_categories) > 0:\n",
        "    for vehicle_category in vehicle_categories:\n",
        "        vehicle_images = list(vehicle_category.glob('*.jpg')) + list(vehicle_category.glob('*.png')) + list(vehicle_category.glob('*.jpeg'))\n",
        "        if len(vehicle_images) > 0:\n",
        "            total_image_count += len(vehicle_images)\n",
        "    print(f\"Total number of vehicle images found: {total_image_count}\")\n",
        "else:\n",
        "    print(\"No vehicle categories (subfolders) found in the directory.\")\n",
        "\n",
        "set_height, set_width = 180, 180\n",
        "batch_size=32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOalI05gI1e8",
        "outputId": "4ad521f1-ef7d-41f9-843e-963bab2a3c82"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of vehicle images found: 1766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Setup\n",
        "training_images = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    loaded_data,\n",
        "    subset=\"training\",\n",
        "    validation_split = 0.25,\n",
        "    seed=18,\n",
        "    image_size=(set_height, set_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "validation_images = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    loaded_data,\n",
        "    subset=\"validation\",  # Use \"validation\" for the validation subset\n",
        "    validation_split=0.25,  # Adjust this value as needed\n",
        "    seed=18,\n",
        "    image_size=(set_height, set_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "vehicle_classes = training_images.class_names\n",
        "print(vehicle_classes)\n",
        "dataset_classes = 2\n",
        "from tensorflow.keras.models import Sequential\n",
        "model=Sequential([])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyH3NcT4I1hY",
        "outputId": "6fd4f6e3-6b36-4414-a158-e2cc22f66e1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1766 files belonging to 2 classes.\n",
            "Using 1325 files for training.\n",
            "Found 1766 files belonging to 2 classes.\n",
            "Using 441 files for validation.\n",
            "['Bikes', 'Cars']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_cnn_model(dropout_rate):\n",
        "    model = Sequential([\n",
        "        layers.experimental.preprocessing.Rescaling(1./255, input_shape=(set_height, set_width, 3)),\n",
        "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),  # Add dropout layer with the specified rate\n",
        "        layers.Dense(dataset_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "r48Hm2lVL07h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a list of dropout rates to try\n",
        "dropout_rates = [0.0, 0.2, 0.4]\n",
        "epochs=10\n",
        "# Store training history for each model\n",
        "training_histories = []\n",
        "\n",
        "# Train models with different dropout rates\n",
        "for dropout_rate in dropout_rates:\n",
        "    print(f\"Training model with dropout rate: {dropout_rate}\")\n",
        "    model = create_cnn_model(dropout_rate)\n",
        "    history = model.fit(\n",
        "        training_images,\n",
        "        validation_data=validation_images,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    training_histories.append((dropout_rate,history))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYpqqGL7L09i",
        "outputId": "886520bc-5ad3-4e92-f5f5-fa1643425e54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with dropout rate: 0.0\n",
            "Epoch 1/10\n",
            "42/42 [==============================] - 59s 1s/step - loss: 0.5247 - accuracy: 0.7472 - val_loss: 0.4792 - val_accuracy: 0.8163\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 58s 1s/step - loss: 0.2244 - accuracy: 0.9200 - val_loss: 0.2314 - val_accuracy: 0.9048\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 57s 1s/step - loss: 0.0870 - accuracy: 0.9713 - val_loss: 0.1078 - val_accuracy: 0.9592\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 55s 1s/step - loss: 0.0466 - accuracy: 0.9842 - val_loss: 0.0998 - val_accuracy: 0.9705\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 52s 1s/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 0.0722 - val_accuracy: 0.9683\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 52s 1s/step - loss: 0.0210 - accuracy: 0.9940 - val_loss: 0.0606 - val_accuracy: 0.9773\n",
            "Epoch 7/10\n",
            "42/42 [==============================] - 56s 1s/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0503 - val_accuracy: 0.9773\n",
            "Epoch 8/10\n",
            "42/42 [==============================] - 57s 1s/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0492 - val_accuracy: 0.9841\n",
            "Epoch 9/10\n",
            "42/42 [==============================] - 57s 1s/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0445 - val_accuracy: 0.9864\n",
            "Epoch 10/10\n",
            "42/42 [==============================] - 56s 1s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9796\n",
            "Training model with dropout rate: 0.2\n",
            "Epoch 1/10\n",
            "42/42 [==============================] - 60s 1s/step - loss: 0.4794 - accuracy: 0.7660 - val_loss: 0.1667 - val_accuracy: 0.9274\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 55s 1s/step - loss: 0.1020 - accuracy: 0.9683 - val_loss: 0.0801 - val_accuracy: 0.9683\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 58s 1s/step - loss: 0.0513 - accuracy: 0.9842 - val_loss: 0.0551 - val_accuracy: 0.9796\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 58s 1s/step - loss: 0.0516 - accuracy: 0.9834 - val_loss: 0.0759 - val_accuracy: 0.9751\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.0175 - accuracy: 0.9940 - val_loss: 0.3146 - val_accuracy: 0.9320\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.0200 - accuracy: 0.9955 - val_loss: 0.1461 - val_accuracy: 0.9615\n",
            "Epoch 7/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0502 - val_accuracy: 0.9841\n",
            "Epoch 8/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0990 - val_accuracy: 0.9683\n",
            "Epoch 9/10\n",
            "42/42 [==============================] - 55s 1s/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0642 - val_accuracy: 0.9864\n",
            "Epoch 10/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9796\n",
            "Training model with dropout rate: 0.4\n",
            "Epoch 1/10\n",
            "42/42 [==============================] - 58s 1s/step - loss: 0.7073 - accuracy: 0.6634 - val_loss: 0.4661 - val_accuracy: 0.8163\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 56s 1s/step - loss: 0.3340 - accuracy: 0.8732 - val_loss: 0.2254 - val_accuracy: 0.9320\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.1826 - accuracy: 0.9419 - val_loss: 0.2034 - val_accuracy: 0.9229\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 58s 1s/step - loss: 0.1244 - accuracy: 0.9555 - val_loss: 0.1237 - val_accuracy: 0.9660\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 54s 1s/step - loss: 0.0595 - accuracy: 0.9842 - val_loss: 0.0883 - val_accuracy: 0.9660\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 53s 1s/step - loss: 0.0297 - accuracy: 0.9917 - val_loss: 0.3452 - val_accuracy: 0.9048\n",
            "Epoch 7/10\n",
            "42/42 [==============================] - 57s 1s/step - loss: 0.0663 - accuracy: 0.9743 - val_loss: 0.0788 - val_accuracy: 0.9773\n",
            "Epoch 8/10\n",
            "42/42 [==============================] - 56s 1s/step - loss: 0.0193 - accuracy: 0.9955 - val_loss: 0.0806 - val_accuracy: 0.9751\n",
            "Epoch 9/10\n",
            "42/42 [==============================] - 55s 1s/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0852 - val_accuracy: 0.9819\n",
            "Epoch 10/10\n",
            "42/42 [==============================] - 56s 1s/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.1080 - val_accuracy: 0.9796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IscZd11yI1kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yx80p8CBI1mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8yWJRKYI1oy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}